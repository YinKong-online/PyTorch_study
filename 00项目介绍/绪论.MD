### 写在前面

1956年，一群科学家齐聚在达特茅斯开了一场会议，只有十来个人参加，也只持续了一个月，看似一场很小的会议，却深刻地改变了未来几十年世界的样貌。这场会议的中心议题是：如何制造一种可以不断学习并且模拟人类智能的机器，而它的产物便是——**人工智能（Artificial Intelligence, AI）**。

**人工智能（AI）**，顾名思义是人工搭建起来的智能。“人工”很好理解，但“智能”又是什么呢？

智能本质上是针对不同情景做出针对性处理的能力。而**人工智能（AI）**，则是让人类搭建起一套可以根据不同输入内容给出针对性输出内容的系统，输出可以是动作、语言，也可以是一种判断。

这就需要我们找到这样一个“黑箱”，一个建立输入与输出对应关系的“函数”。正如图灵测试所定义的：若一个人无法辨别聊天对象是人还是机器，那么这台机器即可被认为具有智能。函数的特性是确定的，给定输入便有固定输出，无论操作者是谁，相同输入总能得到相同结果。

现在的问题是如何做出这样的“黑箱”。科学家们给出了很多思路。比如，一些人从数学的严谨形式化推理体系（离散数学）中获得灵感，认为智能可以通过符号的逻辑推理来模拟，这便是**符号主义（Symbolism）**。举个例子：若A代表“阴天”，B代表“湿度大于70%”，C代表“会下雨”，那么当A和B都为真时，C就为真（即A∧B→C）。人类的许多思考过程，本质上就是这样的逻辑推理。因此，符号主义者相信智能正是一套处理规则和符号的计算系统。这个系统将人类已有的知识和逻辑储存下来，通过不断的逻辑推演，就能实现接近人类的智能。

符号主义最成功的应用是**专家系统（Expert System）**。它学习人类专家的各种知识和规则，根据这些规则对输入信息进行推理，从而给出预测或决策，曾在金融分析、疾病诊断等领域大放异彩。

但随着时间推移，符号主义的缺陷逐渐暴露：现实世界中并没有那么多清晰的规则；同一个问题，不同专家可能给出不同甚至相反的答案；更关键的是，专家系统的能力上限取决于人类专家，无法超越人类，且一旦设定，水平就固定不变，无法像人类一样通过经验积累提升。

正因如此，从20世纪70年代开始，另一个**人工智能（AI）**流派逐渐兴起——**机器学习（Machine Learning）**。这一流派不追求AI一开始就有多高的水平，而是允许它通过学习不断进步。

人们只需要一个具备学习能力的“黑箱”和足够的训练数据，通过“奖励正确、惩罚错误”的机制（类似巴甫洛夫的条件反射实验），就能让这个“黑箱”变得越来越智能。

理解这个原理后，剩下三个核心问题：如何设计“黑箱”（模型结构）、如何定义奖惩（损失函数）、如何高效学习（训练过程）。这三个问题分别对应了现代**人工智能（AI）**的基础框架。

先来思考第一个问题：如何设计具备学习能力的“黑箱”。这时不得不提到另一个**人工智能（AI）**流派——**联结主义（Connectionism）**。联结主义者认为，大自然进化已经给出了实现智能的标准答案——人类的大脑。只需通过仿生方式模拟神经元的功能及其相互联系，就能构建出具有智慧的**人工智能（AI）**系统。

为理解这个思想，我们可以举一个简单的例子：当你看到一个苹果时，你知道它是苹果，是因为它具备红色、直径约10厘米、球形、有香气等特征。你对苹果的认知，正是由这些不同维度的特征值叠加而成的。我们区分不同物体，正是依靠这些维度特征值的差异。

如果我们记住这些特征值，那么遇到新水果时，只需将其特征值与已知水果的特征值进行比较，若与苹果的特征值相近，就可以认为它是苹果。这便是一种简单的智能。

这给了我们启示：可以搭建这样一个“黑箱”，将所有维度（如颜色、大小等）的值作为输入，为每个维度分配不同的权重（系数），将这些加权后的特征值相加得到一个总分，再设置一个阈值。若总分高于阈值，就“激活”输出。用公式表示即：w₁x₁ + w₂x₂ + w₃x₃ + … - b > 0。

这种模式识别算法模型，便是1956年最早提出的**感知机（Perceptron）**。

细心的你会发现，感知机的结构与神经元非常相似——通过接收多个信号，决定是否激活神经元的活动。事实上，感知机模型的提出更早，早在1943年就有科学家发表过相关论文。

当世界上第一台感知机问世后，人们曾过于乐观，认为只要增加神经元数量就能创造出强大的**人工智能（AI）**。然而，正如1900年物理学遇到的瓶颈，这个过于简单的模型根本无法承载如此重任。

1969年，马文·明斯基在《感知机》一书中指出了感知机的致命缺陷：它只能解决线性可分问题，连简单的**异或运算（XOR Operation）**（输入相同为0，不同为1）都无法完成。

这是因为感知机的数学模型本质是线性分类器，只能用直线划分数据。而异或问题需要曲线划分，感知机根本无法做到。

这一发现使联结主义陷入低谷，此后二三十年里，**神经网络（Neural Network）**几乎成了“伪科学”的代名词。但仍有一些科学家坚持探索，为后来的**深度学习（Deep Learning）**奠定了基础。

这些科学家思考：既然单个神经元无法处理异或问题，何不增加神经元的数量和层次？于是，他们设计出**多层感知机（Multi-Layer Perceptron, MLP）**——让第一层感知机的输出作为第二层的输入，层层传递，最终得到结果。

多层感知机的层数远不止两层，每层的输入输出也不止两个。这些神经元层层叠加，便组成了**神经网络（Neural Network）**。

现代科学已经证明，只要**神经网络（Neural Network）**的深度和宽度足够大，理论上可以模拟任何函数。

回过头看，人类的大脑也是由大量神经元连接而成的。正如我们最初的目标——模拟人类智能，**人工智能（AI）**的形态自然也应是对人类大脑的模拟。

进一步研究发现，人类大脑中并非所有神经元都相互连接，有些不必要的连接可以省略。这一发现催生了后来各种各样的神经网络模型，如**卷积神经网络（Convolutional Neural Network, CNN）**、**残差网络（Residual Network, ResNet）**等。由于输入数据的特性不同，识别过程中不需要所有神经元全连接，而是可以根据数据特性进行选择性连接。如何设计这些连接方式，成为**深度学习（Deep Learning）**的重要研究领域——神经网络结构设计。

再来看第二个问题：如何定义奖惩。训练巴甫洛夫的狗时，我们可以用骨头奖励，或用没有骨头惩罚。那么**神经网络（Neural Network）**的奖惩机制是什么？其实，奖惩的目的是让网络学习到正确的参数。如何找到这些参数的最优值，便是数学中的**梯度下降（Gradient Descent）**方法（详见本章节《数学基础.MD》）。

第三个问题：机器如何建立输入与输出的对应关系。这是一个相当“玄学”的问题。训练机器就像学生做练习题——通过大量练习，就能举一反三解决类似问题。当问学生为什么这样解题时，他可能会说“我做过类似的题，感觉应该用这个公式”。机器建立的对应关系同样难以解释，甚至我们人类自己也不清楚自己是如何建立某些认知关系的（至少在本章写作时，这一问题尚未完全解决）。

这种对应关系的建立非常抽象。例如，现在很多人会把“篮球”和“鸡”联系到一起（网络迷因）。**神经网络（Neural Network）**能领悟到什么样的对应关系，完全取决于训练数据。它可能会从数据中学习到一些看似无关的关联。

有人因此认为**神经网络（Neural Network）**存在缺陷。但在我看来，这种“奇怪”关联的建立，恰如心理学中的“幸存者偏差”——人类会因为见到更多的“幸存者”而对事物产生错误判断。**神经网络（Neural Network）**产生奇怪关联的部分原因，也可能是因为它所接触的数据本身就包含“幸存者偏差”，导致它产生了类似的认知偏差（当然，还有部分原因可能源于模型本身的特性，就像人与人之间的差异——有些人能理解抽象的梗，有些人则不能）。

以上便是**人工智能（AI）**的发展历程。在本项目的后续章节中，我们将详细讲解各种**神经网络（Neural Network）**模型。